{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from models.attention import multihead_model\n",
    "from models.selfattention import selfattention_model, selfattention_model_modified\n",
    "from models.baseline_model import baseline, baseline_residual\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from data_utility.file_utility import FileUtility\n",
    "from data_utility.labeling_utility import LabelingData\n",
    "import itertools\n",
    "from data_utility.feedgenerator import train_batch_generator_408, validation_batch_generator_408\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformer_attention(epochs=10, setting_name='attention', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = multihead_model(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF, filter_size=256)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results/' + setting_name + params + '/history', h.history)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling utility object created..\n",
      "Training y encoded shape is  (5534, 700)\n",
      "Maximum sequence length is 700\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 408)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "onehot (Lambda)                 (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequenceprofile (Lambda)        (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 42)     0           onehot[0][0]                     \n",
      "                                                                 sequenceprofile[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_input (BatchNormaliza (None, None, 42)     168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, None, 256)    32512       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, None, 256)    54016       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv1D)                  (None, None, 256)    75520       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv1D)                  (None, None, 256)    118528      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv1D)                  (None, None, 256)    226048      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv1 (BatchNormaliz (None, None, 256)    1024        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv2 (BatchNormaliz (None, None, 256)    1024        conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv3 (BatchNormaliz (None, None, 256)    1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv4 (BatchNormaliz (None, None, 256)    1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv5 (BatchNormaliz (None, None, 256)    1024        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 1322)   0           batchnorm_input[0][0]            \n",
      "                                                                 batch_norm_conv1[0][0]           \n",
      "                                                                 batch_norm_conv2[0][0]           \n",
      "                                                                 batch_norm_conv3[0][0]           \n",
      "                                                                 batch_norm_conv4[0][0]           \n",
      "                                                                 batch_norm_conv5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1322)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 500)    661500      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_1 (Position (None, None, 500)    0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, None, 400)    600000      position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 400)    0           attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 2000)   11216000    dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 9)      18009       bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 13,007,421\n",
      "Trainable params: 13,004,777\n",
      "Non-trainable params: 2,644\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 66s 761ms/step - loss: 1.4927 - weighted_acc: 0.5877 - val_loss: 1.3311 - val_weighted_acc: 0.6218\n",
      "\n",
      "Epoch 00001: val_weighted_acc improved from -inf to 0.62181, saving model to results/multihead_lstm__cnn_timemultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-01-0.588-0.622.hdf5\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 1.1722 - weighted_acc: 0.6693 - val_loss: 1.2233 - val_weighted_acc: 0.6296\n",
      "\n",
      "Epoch 00002: val_weighted_acc improved from 0.62181 to 0.62958, saving model to results/multihead_lstm__cnn_timemultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-02-0.670-0.630.hdf5\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 19s 224ms/step - loss: 1.0806 - weighted_acc: 0.6818 - val_loss: 1.1762 - val_weighted_acc: 0.6346\n",
      "\n",
      "Epoch 00003: val_weighted_acc improved from 0.62958 to 0.63455, saving model to results/multihead_lstm__cnn_timemultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-03-0.682-0.635.hdf5\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 19s 223ms/step - loss: 1.0422 - weighted_acc: 0.6865 - val_loss: 1.1529 - val_weighted_acc: 0.6382\n",
      "\n",
      "Epoch 00004: val_weighted_acc improved from 0.63455 to 0.63824, saving model to results/multihead_lstm__cnn_timemultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-04-0.687-0.638.hdf5\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 1.0341 - weighted_acc: 0.6869 - val_loss: 1.1409 - val_weighted_acc: 0.6438\n",
      "\n",
      "Epoch 00005: val_weighted_acc improved from 0.63824 to 0.64378, saving model to results/multihead_lstm__cnn_timemultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-05-0.687-0.644.hdf5\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 20s 224ms/step - loss: 1.0497 - weighted_acc: 0.6844 - val_loss: 1.1598 - val_weighted_acc: 0.6418\n",
      "\n",
      "Epoch 00006: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 20s 224ms/step - loss: 1.0707 - weighted_acc: 0.6827 - val_loss: 1.1961 - val_weighted_acc: 0.6350\n",
      "\n",
      "Epoch 00007: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 19s 224ms/step - loss: 1.0993 - weighted_acc: 0.6793 - val_loss: 1.2121 - val_weighted_acc: 0.6380\n",
      "\n",
      "Epoch 00008: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 19s 223ms/step - loss: 1.1459 - weighted_acc: 0.6737 - val_loss: 1.2635 - val_weighted_acc: 0.6334\n",
      "\n",
      "Epoch 00009: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 1.1928 - weighted_acc: 0.6692 - val_loss: 1.3046 - val_weighted_acc: 0.6315\n",
      "\n",
      "Epoch 00010: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 1.2803 - weighted_acc: 0.6581 - val_loss: 1.4061 - val_weighted_acc: 0.6195\n",
      "\n",
      "Epoch 00011: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 19s 223ms/step - loss: 1.3443 - weighted_acc: 0.6560 - val_loss: 1.4791 - val_weighted_acc: 0.6107\n",
      "\n",
      "Epoch 00012: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 1.4301 - weighted_acc: 0.6473 - val_loss: 1.5947 - val_weighted_acc: 0.5992\n",
      "\n",
      "Epoch 00013: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 19s 221ms/step - loss: 1.4752 - weighted_acc: 0.6471 - val_loss: 1.6180 - val_weighted_acc: 0.6003\n",
      "\n",
      "Epoch 00014: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 1.5457 - weighted_acc: 0.6424 - val_loss: 1.6913 - val_weighted_acc: 0.5955\n",
      "\n",
      "Epoch 00015: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 1.6536 - weighted_acc: 0.6311 - val_loss: 1.7770 - val_weighted_acc: 0.5963\n",
      "\n",
      "Epoch 00016: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 19s 221ms/step - loss: 1.6919 - weighted_acc: 0.6385 - val_loss: 1.8127 - val_weighted_acc: 0.5980\n",
      "\n",
      "Epoch 00017: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 19s 223ms/step - loss: 1.7355 - weighted_acc: 0.6339 - val_loss: 1.8561 - val_weighted_acc: 0.5938\n",
      "\n",
      "Epoch 00018: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 19s 221ms/step - loss: 1.8494 - weighted_acc: 0.6183 - val_loss: 2.0353 - val_weighted_acc: 0.5675\n",
      "\n",
      "Epoch 00019: val_weighted_acc did not improve from 0.64378\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 19s 222ms/step - loss: 2.0282 - weighted_acc: 0.5967 - val_loss: 2.1802 - val_weighted_acc: 0.5573\n",
      "\n",
      "Epoch 00020: val_weighted_acc did not improve from 0.64378\n"
     ]
    }
   ],
   "source": [
    "run_transformer_attention(epochs=300, setting_name='multihead_lstm__cnn_time', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=15, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selfattention_modified(epochs=10, setting_name='attention', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = selfattention_model_modified(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF, filter_size=256)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results/' + setting_name + params + '/history', h.history)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling utility object created..\n",
      "Training y encoded shape is  (5534, 700)\n",
      "Maximum sequence length is 700\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 408)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "onehot (Lambda)                 (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequenceprofile (Lambda)        (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 42)     0           onehot[0][0]                     \n",
      "                                                                 sequenceprofile[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "instance_normalization_1 (Insta (None, None, 42)     2           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, None, 256)    32512       instance_normalization_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, None, 256)    54016       instance_normalization_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv1D)                  (None, None, 256)    75520       instance_normalization_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv1D)                  (None, None, 256)    118528      instance_normalization_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv1D)                  (None, None, 256)    226048      instance_normalization_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv1 (BatchNormaliz (None, None, 256)    1024        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv2 (BatchNormaliz (None, None, 256)    1024        conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv3 (BatchNormaliz (None, None, 256)    1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv4 (BatchNormaliz (None, None, 256)    1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv5 (BatchNormaliz (None, None, 256)    1024        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 1322)   0           instance_normalization_1[0][0]   \n",
      "                                                                 batch_norm_conv1[0][0]           \n",
      "                                                                 batch_norm_conv2[0][0]           \n",
      "                                                                 batch_norm_conv3[0][0]           \n",
      "                                                                 batch_norm_conv4[0][0]           \n",
      "                                                                 batch_norm_conv5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropoutonconvs (Dropout)        (None, None, 1322)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "denseonconvs (Dense)            (None, None, 1000)   1323000     dropoutonconvs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_dense (BatchNormaliz (None, None, 1000)   4000        denseonconvs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1000)   0           batch_norm_dense[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 2000)   16016000    dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Attention (SeqSelfAttention)    (None, None, 2000)   4000001     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 2000)   0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 65)     130065      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 9)      594         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,985,406\n",
      "Trainable params: 21,980,846\n",
      "Non-trainable params: 4,560\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 76s 869ms/step - loss: 1.6595 - weighted_acc: 0.6017 - val_loss: 1.4554 - val_weighted_acc: 0.6348\n",
      "\n",
      "Epoch 00001: val_weighted_acc improved from -inf to 0.63476, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-01-0.602-0.635.hdf5\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 1.2326 - weighted_acc: 0.6847 - val_loss: 1.2941 - val_weighted_acc: 0.6276\n",
      "\n",
      "Epoch 00002: val_weighted_acc did not improve from 0.63476\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 1.0407 - weighted_acc: 0.7030 - val_loss: 1.1694 - val_weighted_acc: 0.6343\n",
      "\n",
      "Epoch 00003: val_weighted_acc did not improve from 0.63476\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.9283 - weighted_acc: 0.7142 - val_loss: 1.0800 - val_weighted_acc: 0.6486\n",
      "\n",
      "Epoch 00004: val_weighted_acc improved from 0.63476 to 0.64857, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-04-0.715-0.649.hdf5\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.8714 - weighted_acc: 0.7199 - val_loss: 1.0190 - val_weighted_acc: 0.6649\n",
      "\n",
      "Epoch 00005: val_weighted_acc improved from 0.64857 to 0.66495, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-05-0.720-0.665.hdf5\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.8398 - weighted_acc: 0.7252 - val_loss: 0.9893 - val_weighted_acc: 0.6733\n",
      "\n",
      "Epoch 00006: val_weighted_acc improved from 0.66495 to 0.67328, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-06-0.726-0.673.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.8191 - weighted_acc: 0.7298 - val_loss: 0.9849 - val_weighted_acc: 0.6752\n",
      "\n",
      "Epoch 00007: val_weighted_acc improved from 0.67328 to 0.67522, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-07-0.730-0.675.hdf5\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.8111 - weighted_acc: 0.7328 - val_loss: 0.9719 - val_weighted_acc: 0.6790\n",
      "\n",
      "Epoch 00008: val_weighted_acc improved from 0.67522 to 0.67902, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-08-0.733-0.679.hdf5\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7988 - weighted_acc: 0.7356 - val_loss: 0.9738 - val_weighted_acc: 0.6801\n",
      "\n",
      "Epoch 00009: val_weighted_acc improved from 0.67902 to 0.68011, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-09-0.736-0.680.hdf5\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.7916 - weighted_acc: 0.7385 - val_loss: 0.9622 - val_weighted_acc: 0.6858\n",
      "\n",
      "Epoch 00010: val_weighted_acc improved from 0.68011 to 0.68580, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-10-0.739-0.686.hdf5\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7839 - weighted_acc: 0.7409 - val_loss: 0.9492 - val_weighted_acc: 0.6895\n",
      "\n",
      "Epoch 00011: val_weighted_acc improved from 0.68580 to 0.68951, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-11-0.741-0.690.hdf5\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.7792 - weighted_acc: 0.7435 - val_loss: 0.9598 - val_weighted_acc: 0.6875\n",
      "\n",
      "Epoch 00012: val_weighted_acc did not improve from 0.68951\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.7647 - weighted_acc: 0.7461 - val_loss: 0.9561 - val_weighted_acc: 0.6876\n",
      "\n",
      "Epoch 00013: val_weighted_acc did not improve from 0.68951\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 0.7838 - weighted_acc: 0.7476 - val_loss: 0.9787 - val_weighted_acc: 0.6881\n",
      "\n",
      "Epoch 00014: val_weighted_acc did not improve from 0.68951\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 0.7628 - weighted_acc: 0.7512 - val_loss: 0.9515 - val_weighted_acc: 0.6872\n",
      "\n",
      "Epoch 00015: val_weighted_acc did not improve from 0.68951\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7359 - weighted_acc: 0.7558 - val_loss: 0.9426 - val_weighted_acc: 0.6895\n",
      "\n",
      "Epoch 00016: val_weighted_acc improved from 0.68951 to 0.68952, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-16-0.756-0.690.hdf5\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7340 - weighted_acc: 0.7576 - val_loss: 0.9597 - val_weighted_acc: 0.6893\n",
      "\n",
      "Epoch 00017: val_weighted_acc did not improve from 0.68952\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7482 - weighted_acc: 0.7587 - val_loss: 0.9654 - val_weighted_acc: 0.6920\n",
      "\n",
      "Epoch 00018: val_weighted_acc improved from 0.68952 to 0.69201, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-18-0.759-0.692.hdf5\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7218 - weighted_acc: 0.7629 - val_loss: 0.9531 - val_weighted_acc: 0.6931\n",
      "\n",
      "Epoch 00019: val_weighted_acc improved from 0.69201 to 0.69305, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-19-0.763-0.693.hdf5\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7343 - weighted_acc: 0.7643 - val_loss: 0.9768 - val_weighted_acc: 0.6948\n",
      "\n",
      "Epoch 00020: val_weighted_acc improved from 0.69305 to 0.69479, saving model to results/self-attention_withconv_2drops_dense_instance_norm_new_selfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-20-0.765-0.695.hdf5\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7127 - weighted_acc: 0.7681 - val_loss: 0.9469 - val_weighted_acc: 0.6941\n",
      "\n",
      "Epoch 00021: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7006 - weighted_acc: 0.7714 - val_loss: 0.9588 - val_weighted_acc: 0.6911\n",
      "\n",
      "Epoch 00022: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.7074 - weighted_acc: 0.7729 - val_loss: 0.9718 - val_weighted_acc: 0.6915\n",
      "\n",
      "Epoch 00023: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7043 - weighted_acc: 0.7744 - val_loss: 0.9684 - val_weighted_acc: 0.6910\n",
      "\n",
      "Epoch 00024: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.6862 - weighted_acc: 0.7766 - val_loss: 1.0016 - val_weighted_acc: 0.6889\n",
      "\n",
      "Epoch 00025: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.6749 - weighted_acc: 0.7791 - val_loss: 1.1632 - val_weighted_acc: 0.6883\n",
      "\n",
      "Epoch 00026: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.6672 - weighted_acc: 0.7830 - val_loss: 0.9799 - val_weighted_acc: 0.6878\n",
      "\n",
      "Epoch 00027: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.6674 - weighted_acc: 0.7862 - val_loss: 1.0058 - val_weighted_acc: 0.6861\n",
      "\n",
      "Epoch 00028: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.6621 - weighted_acc: 0.7893 - val_loss: 0.9827 - val_weighted_acc: 0.6880\n",
      "\n",
      "Epoch 00029: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.6317 - weighted_acc: 0.7930 - val_loss: 1.0092 - val_weighted_acc: 0.6897\n",
      "\n",
      "Epoch 00030: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7109 - weighted_acc: 0.7895 - val_loss: 1.0910 - val_weighted_acc: 0.6894\n",
      "\n",
      "Epoch 00031: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.6976 - weighted_acc: 0.7911 - val_loss: 1.0067 - val_weighted_acc: 0.6859\n",
      "\n",
      "Epoch 00032: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 0.6296 - weighted_acc: 0.7977 - val_loss: 0.9697 - val_weighted_acc: 0.6896\n",
      "\n",
      "Epoch 00033: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.6089 - weighted_acc: 0.8011 - val_loss: 0.9811 - val_weighted_acc: 0.6881\n",
      "\n",
      "Epoch 00034: val_weighted_acc did not improve from 0.69479\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.6027 - weighted_acc: 0.8044 - val_loss: 0.9839 - val_weighted_acc: 0.6894\n",
      "\n",
      "Epoch 00035: val_weighted_acc did not improve from 0.69479\n"
     ]
    }
   ],
   "source": [
    "model=run_selfattention_modified(epochs=300, setting_name='self-attention_withconv_2drops_dense_instance_norm_new_', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=15, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
