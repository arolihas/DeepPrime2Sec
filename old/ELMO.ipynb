{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Replicated the architecture from\n",
    "   Deep Recurrent Conditional Random Field Network for Protein Secondary Prediction\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "from keras.models import Model\n",
    "from keras.layers import Masking, Lambda, Dense, LSTM, CuDNNLSTM, Reshape, Flatten, Activation, RepeatVector, Permute, Bidirectional, Embedding, Input, Dropout, concatenate, Masking, Conv1D, \\\n",
    "    multiply, BatchNormalization, merge\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras import regularizers\n",
    "from models.layers import ChainCRF, slice_tensor\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from data_utility.file_utility import FileUtility\n",
    "\n",
    "\n",
    "\n",
    "def elmo_embedding_model(n_classes, convs=[3,5,7], dense_size=200, lstm_size=400, drop_lstm=0.5, features_to_use=['onehot','sequence_profile'], use_CRF=False):\n",
    "    '''\n",
    "    :param max_length:\n",
    "    :param n_classes:\n",
    "    :param embedding_layer:\n",
    "    :param vocab_size:\n",
    "    :return:\n",
    "    '''\n",
    "    visible = Input(shape=(None,408))\n",
    "    biophysical = slice_tensor(2,0,16,name='biophysicalfeatures')(visible)\n",
    "    embedding = slice_tensor(2,16,66,name='skipgramembd')(visible)\n",
    "    onehot = slice_tensor(2,66,87,name='onehot')(visible)\n",
    "    prof = slice_tensor(2,87,108,name='sequenceprofile')(visible)\n",
    "    elmo = slice_tensor(2,108,408,name='elmo')(visible)\n",
    "\n",
    "    elme_embedding = ElmoEmbeddingLayer()(onehot)\n",
    "    input_dict={'sequence_profile':prof, 'onehot':onehot,'embedding':embedding,'elmo':elmo,'biophysical':biophysical,'elme_embedding':elme_embedding}\n",
    "\n",
    "    # create input\n",
    "    features=[]\n",
    "    for feature in features_to_use:\n",
    "        features.append(input_dict[feature])\n",
    "\n",
    "\n",
    "    if len(features_to_use)==1:\n",
    "        conclayers=features\n",
    "        input=BatchNormalization(name='batchnorm_input') (features[0])\n",
    "    else:\n",
    "        input =BatchNormalization(name='batchnorm_input') (concatenate(features))\n",
    "        conclayers=[input]\n",
    "\n",
    "    # filter_size\n",
    "    filter_size=256#int(int(input.shape[2])*0.8)\n",
    "\n",
    "    # performing the conlvolutions\n",
    "    for idx,conv in enumerate(convs):\n",
    "        idx=str(idx+1)\n",
    "        conclayers.append(BatchNormalization(name='batch_norm_conv'+idx) (Conv1D(filter_size, conv, activation=\"relu\", padding=\"same\", name='conv'+idx,\n",
    "                   kernel_regularizer=regularizers.l2(0.001))(input)))\n",
    "\n",
    "    conc = concatenate(conclayers)\n",
    "\n",
    "    if drop_lstm > 0:\n",
    "        drop0 = Dropout(drop_lstm,name='dropoutonconvs')(conc)\n",
    "        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop0)\n",
    "    else:\n",
    "        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)\n",
    "\n",
    "    dense_convinpn=BatchNormalization(name='batch_norm_dense') (dense_convinp)\n",
    "\n",
    "    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)\n",
    "    #drop1 = Dropout(0.5)(lstm)\n",
    "    attn_out = Attention()(lstm)\n",
    "    \n",
    "    #dense_out = Dense(dense_size, activation='relu')(drop1)\n",
    "\n",
    "    if use_CRF:\n",
    "        timedist = TimeDistributed(Dense(n_classes, name='dense'))(attn_out)\n",
    "        crf = ChainCRF(name=\"crf1\")\n",
    "        crf_output = crf(timedist)\n",
    "        print(crf_output)\n",
    "        print(K.int_shape(crf_output))\n",
    "        model = Model(inputs=visible, outputs=crf_output)\n",
    "        adam=optimizers.Adam(lr=0.001)\n",
    "        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics= ['accuracy'], sample_weight_mode='temporal')\n",
    "    else:\n",
    "        timedist = TimeDistributed(Dense(n_classes, activation='softmax'))(attn_out)\n",
    "        model = Model(inputs=visible, outputs=timedist)\n",
    "        adam=optimizers.Adam(lr=0.001)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, weighted_metrics= ['accuracy'], sample_weight_mode='temporal')\n",
    "    print(model.summary())\n",
    "    return model, 'model#'+'#'.join(features_to_use)+'@conv'+'_'.join([str(c) for c in convs])+'@dense_'+str(dense_size)+'@lstm'+str(lstm_size)+'@droplstm'+str(drop_lstm)+'@filtersize_'+str(filter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utility.labeling_utility import LabelingData\n",
    "import itertools\n",
    "from data_utility.feedgenerator import train_batch_generator_408, validation_batch_generator_408\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "def run_baseline(epochs=10, setting_name='basemodel', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False,filter_size=256):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = elmo_embedding_model(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('baseline_modifications/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('baseline_modifications/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'baseline_modifications/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        lenx(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('baseline_modifications/' + setting_name + params + '/history', h.history)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling utility object created..\n",
      "Training y encoded shape is  (5534, 700)\n",
      "Maximum sequence length is 700\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'init_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4e982048105c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model=run_baseline(epochs=1000, setting_name='baseline_no_cnn', gpu='0', train_batch_size=64,\n\u001b[0;32m----> 2\u001b[0;31m                  test_batch_size=100, patience=50, features_to_use=['onehot', 'sequence_profile','elme_embedding'], convs=[3,5,7,11,21], dense_size=100, lstm_size=100,use_CRF=False, filter_size=10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-2926a1f1bb9d>\u001b[0m in \u001b[0;36mrun_baseline\u001b[0;34m(epochs, setting_name, gpu, train_batch_size, test_batch_size, patience, features_to_use, convs, dense_size, lstm_size, use_CRF, filter_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     model, params = elmo_embedding_model(LD.n_classes, features_to_use=features_to_use, convs=convs,\n\u001b[0;32m---> 19\u001b[0;31m                              dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# output directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f7d9fd387a6c>\u001b[0m in \u001b[0;36melmo_embedding_model\u001b[0;34m(n_classes, convs, dense_size, lstm_size, drop_lstm, features_to_use, use_CRF)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m108\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m408\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'elmo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0melme_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElmoEmbeddingLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0minput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sequence_profile'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'onehot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'elmo'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0melmo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'biophysical'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbiophysical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'elme_embedding'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0melme_embedding\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3_softwaretest/envs/keras/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f7d9fd387a6c>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n\u001b[0;32m---> 21\u001b[0;31m                                name=\"{}_module\".format(self.name))\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"^{}_module/.*\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3_softwaretest/envs/keras/lib/python3.6/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    167\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m           tags=self._tags)\n\u001b[0m\u001b[1;32m    170\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3_softwaretest/envs/keras/lib/python3.6/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[0;34m(self, name, trainable, tags)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3_softwaretest/envs/keras/lib/python3.6/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# when creating the Module state. This use case has showed up in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;31m# TPU training code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'init_scope'"
     ]
    }
   ],
   "source": [
    "model=run_baseline(epochs=1000, setting_name='baseline_no_cnn', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=50, features_to_use=['onehot', 'sequence_profile','elme_embedding'], convs=[3,5,7,11,21], dense_size=100, lstm_size=100,use_CRF=False, filter_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
