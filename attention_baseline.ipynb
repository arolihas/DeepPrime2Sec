{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from models.att_models import att_baseline\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from data_utility.file_utility import FileUtility\n",
    "from data_utility.labeling_utility import LabelingData\n",
    "import itertools\n",
    "from data_utility.feedgenerator import train_batch_generator_408, validation_batch_generator_408\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(epochs=10, setting_name='basemodel', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10,lstm_size =1000, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200,use_CRF=False,filter_size=256):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = att_baseline(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, use_CRF=use_CRF,lstm_size =lstm_size, filter_size=filter_size)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results_att/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results_att/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results_att/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results_att/' + setting_name + params + '/history', h.history)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling utility object created..\n",
      "Training y encoded shape is  (5534, 700)\n",
      "Maximum sequence length is 700\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 408)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "onehot (Lambda)                 (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequenceprofile (Lambda)        (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 42)     0           onehot[0][0]                     \n",
      "                                                                 sequenceprofile[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_input (BatchNormaliza (None, None, 42)     168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, None, 256)    32512       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, None, 256)    54016       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv1D)                  (None, None, 256)    75520       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv1D)                  (None, None, 256)    118528      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv1D)                  (None, None, 256)    226048      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv1 (BatchNormaliz (None, None, 256)    1024        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv2 (BatchNormaliz (None, None, 256)    1024        conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv3 (BatchNormaliz (None, None, 256)    1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv4 (BatchNormaliz (None, None, 256)    1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv5 (BatchNormaliz (None, None, 256)    1024        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 1322)   0           batchnorm_input[0][0]            \n",
      "                                                                 batch_norm_conv1[0][0]           \n",
      "                                                                 batch_norm_conv2[0][0]           \n",
      "                                                                 batch_norm_conv3[0][0]           \n",
      "                                                                 batch_norm_conv4[0][0]           \n",
      "                                                                 batch_norm_conv5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropoutonconvs (Dropout)        (None, None, 1322)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "denseonconvs (Dense)            (None, None, 1000)   1323000     dropoutonconvs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_dense (BatchNormaliz (None, None, 1000)   4000        denseonconvs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 2000)   16016000    batch_norm_dense[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Attention (SeqSelfAttention)    (None, None, 2000)   128065      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 9)      18009       Attention[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18,000,986\n",
      "Trainable params: 17,996,342\n",
      "Non-trainable params: 4,644\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "87/87 [==============================] - 74s 852ms/step - loss: 1.4877 - weighted_acc: 0.5653 - val_loss: 1.2825 - val_weighted_acc: 0.6062\n",
      "\n",
      "Epoch 00001: val_weighted_acc improved from -inf to 0.60618, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-01-0.565-0.606.hdf5\n",
      "Epoch 2/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 1.0413 - weighted_acc: 0.6837 - val_loss: 1.1610 - val_weighted_acc: 0.6101\n",
      "\n",
      "Epoch 00002: val_weighted_acc improved from 0.60618 to 0.61013, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-02-0.684-0.610.hdf5\n",
      "Epoch 3/1000\n",
      "87/87 [==============================] - 27s 313ms/step - loss: 0.9169 - weighted_acc: 0.7053 - val_loss: 1.1064 - val_weighted_acc: 0.6254\n",
      "\n",
      "Epoch 00003: val_weighted_acc improved from 0.61013 to 0.62542, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-03-0.706-0.625.hdf5\n",
      "Epoch 4/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.8500 - weighted_acc: 0.7202 - val_loss: 1.0502 - val_weighted_acc: 0.6470\n",
      "\n",
      "Epoch 00004: val_weighted_acc improved from 0.62542 to 0.64699, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-04-0.721-0.647.hdf5\n",
      "Epoch 5/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.8120 - weighted_acc: 0.7310 - val_loss: 0.9998 - val_weighted_acc: 0.6634\n",
      "\n",
      "Epoch 00005: val_weighted_acc improved from 0.64699 to 0.66336, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-05-0.731-0.663.hdf5\n",
      "Epoch 6/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.7855 - weighted_acc: 0.7399 - val_loss: 0.9781 - val_weighted_acc: 0.6729\n",
      "\n",
      "Epoch 00006: val_weighted_acc improved from 0.66336 to 0.67289, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-06-0.740-0.673.hdf5\n",
      "Epoch 7/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.7646 - weighted_acc: 0.7476 - val_loss: 0.9683 - val_weighted_acc: 0.6774\n",
      "\n",
      "Epoch 00007: val_weighted_acc improved from 0.67289 to 0.67741, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-07-0.748-0.677.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.7493 - weighted_acc: 0.7539 - val_loss: 0.9699 - val_weighted_acc: 0.6823\n",
      "\n",
      "Epoch 00008: val_weighted_acc improved from 0.67741 to 0.68232, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-08-0.754-0.682.hdf5\n",
      "Epoch 9/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.7511 - weighted_acc: 0.7553 - val_loss: 0.9678 - val_weighted_acc: 0.6840\n",
      "\n",
      "Epoch 00009: val_weighted_acc improved from 0.68232 to 0.68403, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-09-0.756-0.684.hdf5\n",
      "Epoch 10/1000\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7213 - weighted_acc: 0.7664 - val_loss: 0.9614 - val_weighted_acc: 0.6846\n",
      "\n",
      "Epoch 00010: val_weighted_acc improved from 0.68403 to 0.68457, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-10-0.767-0.685.hdf5\n",
      "Epoch 11/1000\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.6831 - weighted_acc: 0.7785 - val_loss: 0.9660 - val_weighted_acc: 0.6836\n",
      "\n",
      "Epoch 00011: val_weighted_acc did not improve from 0.68457\n",
      "Epoch 12/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.6576 - weighted_acc: 0.7884 - val_loss: 0.9720 - val_weighted_acc: 0.6848\n",
      "\n",
      "Epoch 00012: val_weighted_acc improved from 0.68457 to 0.68478, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-12-0.789-0.685.hdf5\n",
      "Epoch 13/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.6481 - weighted_acc: 0.7935 - val_loss: 0.9786 - val_weighted_acc: 0.6850\n",
      "\n",
      "Epoch 00013: val_weighted_acc improved from 0.68478 to 0.68501, saving model to results_att/attention_additive_model#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-13-0.794-0.685.hdf5\n",
      "Epoch 14/1000\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.6381 - weighted_acc: 0.7981 - val_loss: 0.9936 - val_weighted_acc: 0.6839\n",
      "\n",
      "Epoch 00014: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 15/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.6257 - weighted_acc: 0.8034 - val_loss: 1.0015 - val_weighted_acc: 0.6830\n",
      "\n",
      "Epoch 00015: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 16/1000\n",
      "87/87 [==============================] - 27s 311ms/step - loss: 0.6007 - weighted_acc: 0.8120 - val_loss: 1.0151 - val_weighted_acc: 0.6817\n",
      "\n",
      "Epoch 00016: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 17/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.5782 - weighted_acc: 0.8197 - val_loss: 1.0335 - val_weighted_acc: 0.6822\n",
      "\n",
      "Epoch 00017: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 18/1000\n",
      "87/87 [==============================] - 27s 311ms/step - loss: 0.5579 - weighted_acc: 0.8269 - val_loss: 1.0482 - val_weighted_acc: 0.6820\n",
      "\n",
      "Epoch 00018: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 19/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.5400 - weighted_acc: 0.8333 - val_loss: 1.0600 - val_weighted_acc: 0.6791\n",
      "\n",
      "Epoch 00019: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 20/1000\n",
      "87/87 [==============================] - 27s 316ms/step - loss: 0.5175 - weighted_acc: 0.8419 - val_loss: 1.0698 - val_weighted_acc: 0.6789\n",
      "\n",
      "Epoch 00020: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 21/1000\n",
      "87/87 [==============================] - 27s 311ms/step - loss: 0.5000 - weighted_acc: 0.8489 - val_loss: 1.0789 - val_weighted_acc: 0.6764\n",
      "\n",
      "Epoch 00021: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 22/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.4904 - weighted_acc: 0.8531 - val_loss: 1.1002 - val_weighted_acc: 0.6709\n",
      "\n",
      "Epoch 00022: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 23/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.4738 - weighted_acc: 0.8593 - val_loss: 1.1002 - val_weighted_acc: 0.6722\n",
      "\n",
      "Epoch 00023: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 24/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.4476 - weighted_acc: 0.8683 - val_loss: 1.1361 - val_weighted_acc: 0.6714\n",
      "\n",
      "Epoch 00024: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 25/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.4195 - weighted_acc: 0.8775 - val_loss: 1.1631 - val_weighted_acc: 0.6720\n",
      "\n",
      "Epoch 00025: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 26/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.4006 - weighted_acc: 0.8840 - val_loss: 1.1914 - val_weighted_acc: 0.6726\n",
      "\n",
      "Epoch 00026: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 27/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.3895 - weighted_acc: 0.8881 - val_loss: 1.2023 - val_weighted_acc: 0.6707\n",
      "\n",
      "Epoch 00027: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 28/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.3809 - weighted_acc: 0.8910 - val_loss: 1.2143 - val_weighted_acc: 0.6659\n",
      "\n",
      "Epoch 00028: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 29/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.3678 - weighted_acc: 0.8959 - val_loss: 1.2388 - val_weighted_acc: 0.6659\n",
      "\n",
      "Epoch 00029: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 30/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.3526 - weighted_acc: 0.9014 - val_loss: 1.2574 - val_weighted_acc: 0.6709\n",
      "\n",
      "Epoch 00030: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 31/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.3404 - weighted_acc: 0.9057 - val_loss: 1.2787 - val_weighted_acc: 0.6707\n",
      "\n",
      "Epoch 00031: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 32/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.3305 - weighted_acc: 0.9089 - val_loss: 1.2962 - val_weighted_acc: 0.6650\n",
      "\n",
      "Epoch 00032: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 33/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.3234 - weighted_acc: 0.9111 - val_loss: 1.3357 - val_weighted_acc: 0.6640\n",
      "\n",
      "Epoch 00033: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 34/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.3171 - weighted_acc: 0.9133 - val_loss: 1.3366 - val_weighted_acc: 0.6639\n",
      "\n",
      "Epoch 00034: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 35/1000\n",
      "87/87 [==============================] - 27s 311ms/step - loss: 0.3022 - weighted_acc: 0.9184 - val_loss: 1.3686 - val_weighted_acc: 0.6616\n",
      "\n",
      "Epoch 00035: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 36/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.2886 - weighted_acc: 0.9229 - val_loss: 1.3735 - val_weighted_acc: 0.6628\n",
      "\n",
      "Epoch 00036: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 37/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.2735 - weighted_acc: 0.9275 - val_loss: 1.3935 - val_weighted_acc: 0.6527\n",
      "\n",
      "Epoch 00037: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 38/1000\n",
      "87/87 [==============================] - 28s 317ms/step - loss: 0.2583 - weighted_acc: 0.9328 - val_loss: 1.4155 - val_weighted_acc: 0.6543\n",
      "\n",
      "Epoch 00038: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 39/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.2495 - weighted_acc: 0.9358 - val_loss: 1.4523 - val_weighted_acc: 0.6569\n",
      "\n",
      "Epoch 00039: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 40/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.2427 - weighted_acc: 0.9378 - val_loss: 1.4774 - val_weighted_acc: 0.6621\n",
      "\n",
      "Epoch 00040: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 41/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.2315 - weighted_acc: 0.9419 - val_loss: 1.4867 - val_weighted_acc: 0.6635\n",
      "\n",
      "Epoch 00041: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 42/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 27s 307ms/step - loss: 0.2232 - weighted_acc: 0.9445 - val_loss: 1.5101 - val_weighted_acc: 0.6598\n",
      "\n",
      "Epoch 00042: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 43/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.2132 - weighted_acc: 0.9482 - val_loss: 1.5388 - val_weighted_acc: 0.6575\n",
      "\n",
      "Epoch 00043: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 44/1000\n",
      "87/87 [==============================] - 27s 311ms/step - loss: 0.2127 - weighted_acc: 0.9488 - val_loss: 1.5442 - val_weighted_acc: 0.6554\n",
      "\n",
      "Epoch 00044: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 45/1000\n",
      "87/87 [==============================] - 27s 312ms/step - loss: 0.2080 - weighted_acc: 0.9506 - val_loss: 1.6609 - val_weighted_acc: 0.6483\n",
      "\n",
      "Epoch 00045: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 46/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.2047 - weighted_acc: 0.9520 - val_loss: 1.6144 - val_weighted_acc: 0.6581\n",
      "\n",
      "Epoch 00046: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 47/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.2061 - weighted_acc: 0.9518 - val_loss: 1.6107 - val_weighted_acc: 0.6602\n",
      "\n",
      "Epoch 00047: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 48/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.1989 - weighted_acc: 0.9542 - val_loss: 1.6396 - val_weighted_acc: 0.6543\n",
      "\n",
      "Epoch 00048: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 49/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.1915 - weighted_acc: 0.9561 - val_loss: 1.6668 - val_weighted_acc: 0.6551\n",
      "\n",
      "Epoch 00049: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 50/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.1859 - weighted_acc: 0.9577 - val_loss: 1.6910 - val_weighted_acc: 0.6487\n",
      "\n",
      "Epoch 00050: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 51/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.1788 - weighted_acc: 0.9595 - val_loss: 1.6851 - val_weighted_acc: 0.6486\n",
      "\n",
      "Epoch 00051: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 52/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.1744 - weighted_acc: 0.9610 - val_loss: 1.7446 - val_weighted_acc: 0.6513\n",
      "\n",
      "Epoch 00052: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 53/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.1718 - weighted_acc: 0.9620 - val_loss: 1.7711 - val_weighted_acc: 0.6595\n",
      "\n",
      "Epoch 00053: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 54/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.1719 - weighted_acc: 0.9620 - val_loss: 1.8278 - val_weighted_acc: 0.6635\n",
      "\n",
      "Epoch 00054: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 55/1000\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.1692 - weighted_acc: 0.9631 - val_loss: 1.8148 - val_weighted_acc: 0.6592\n",
      "\n",
      "Epoch 00055: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 56/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.1663 - weighted_acc: 0.9641 - val_loss: 1.7782 - val_weighted_acc: 0.6563\n",
      "\n",
      "Epoch 00056: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 57/1000\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.1605 - weighted_acc: 0.9658 - val_loss: 1.7792 - val_weighted_acc: 0.6527\n",
      "\n",
      "Epoch 00057: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 58/1000\n",
      "87/87 [==============================] - 27s 312ms/step - loss: 0.1509 - weighted_acc: 0.9683 - val_loss: 1.8610 - val_weighted_acc: 0.6514\n",
      "\n",
      "Epoch 00058: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 59/1000\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.1427 - weighted_acc: 0.9710 - val_loss: 1.8568 - val_weighted_acc: 0.6538\n",
      "\n",
      "Epoch 00059: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 60/1000\n",
      "87/87 [==============================] - 28s 322ms/step - loss: 0.1387 - weighted_acc: 0.9717 - val_loss: 1.8705 - val_weighted_acc: 0.6552\n",
      "\n",
      "Epoch 00060: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 61/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.1354 - weighted_acc: 0.9729 - val_loss: 1.9032 - val_weighted_acc: 0.6494\n",
      "\n",
      "Epoch 00061: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 62/1000\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.1324 - weighted_acc: 0.9738 - val_loss: 1.9631 - val_weighted_acc: 0.6515\n",
      "\n",
      "Epoch 00062: val_weighted_acc did not improve from 0.68501\n",
      "Epoch 63/1000\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.1319 - weighted_acc: 0.9741 - val_loss: 1.9453 - val_weighted_acc: 0.6539\n",
      "\n",
      "Epoch 00063: val_weighted_acc did not improve from 0.68501\n"
     ]
    }
   ],
   "source": [
    "model=run_baseline(epochs=1000, setting_name='attention_additive_', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=64, patience=50, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], lstm_size=1000, dense_size=1000, use_CRF=False, filter_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
