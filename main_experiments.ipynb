{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from models.attention import multihead_model\n",
    "from models.selfattention import selfattention_model, selfattention_model_modified\n",
    "from models.baseline_model import baseline, baseline_residual\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from data_utility.file_utility import FileUtility\n",
    "from data_utility.labeling_utility import LabelingData\n",
    "import itertools\n",
    "from data_utility.feedgenerator import train_batch_generator_408, validation_batch_generator_408\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(epochs=10, setting_name='basemodel', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = baseline(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results/' + setting_name + params + '/history', h.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_residual(epochs=10, setting_name='basemodel', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = baseline_residual(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results/' + setting_name + params + '/history', h.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_attention(epochs=10, setting_name='attention', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = selfattention_model(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF, filter_size=256)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results/' + setting_name + params + '/history', h.history)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformer_attention(epochs=10, setting_name='attention', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = multihead_model(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF, filter_size=256)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results/' + setting_name + params + '/history', h.history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline(epochs=30, setting_name='basemodel_no_crf', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_baseline(epochs=30, setting_name='basemodel_with_crf', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling utility object created..\n",
      "Training y encoded shape is  (5534, 700)\n",
      "Maximum sequence length is 700\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 408)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "onehot (Lambda)                 (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequenceprofile (Lambda)        (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 42)     0           onehot[0][0]                     \n",
      "                                                                 sequenceprofile[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_input (BatchNormaliza (None, None, 42)     168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, None, 32)     4064        batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, None, 32)     6752        batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv1D)                  (None, None, 32)     9440        batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv1D)                  (None, None, 32)     14816       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv1D)                  (None, None, 32)     28256       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv1 (BatchNormaliz (None, None, 32)     128         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv2 (BatchNormaliz (None, None, 32)     128         conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv3 (BatchNormaliz (None, None, 32)     128         conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv4 (BatchNormaliz (None, None, 32)     128         conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv5 (BatchNormaliz (None, None, 32)     128         conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 202)    0           batchnorm_input[0][0]            \n",
      "                                                                 batch_norm_conv1[0][0]           \n",
      "                                                                 batch_norm_conv2[0][0]           \n",
      "                                                                 batch_norm_conv3[0][0]           \n",
      "                                                                 batch_norm_conv4[0][0]           \n",
      "                                                                 batch_norm_conv5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropoutonconvs (Dropout)        (None, None, 202)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "denseonconvs (Dense)            (None, None, 1000)   203000      dropoutonconvs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_dense (BatchNormaliz (None, None, 1000)   4000        denseonconvs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 2000)   16008000    batch_norm_dense[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 2000)   0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1000)   2001000     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batchnormseqprof (BatchNormaliz (None, None, 21)     84          sequenceprofile[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1021)   0           dense_1[0][0]                    \n",
      "                                                                 batchnormseqprof[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 9)      9198        concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 18,289,418\n",
      "Trainable params: 18,286,972\n",
      "Non-trainable params: 2,446\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      " 7/87 [=>............................] - ETA: 16:22 - loss: 1.7229 - weighted_acc: 0.4853"
     ]
    }
   ],
   "source": [
    "run_residual(epochs=30, setting_name='residual_no_crf', gpu='1', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=run_attention(epochs=300, setting_name='attention_dropout_before_lstm_', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=15, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling utility object created..\n",
      "Training y encoded shape is  (5534, 700)\n",
      "Maximum sequence length is 700\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 408)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "onehot (Lambda)                 (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequenceprofile (Lambda)        (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 42)     0           onehot[0][0]                     \n",
      "                                                                 sequenceprofile[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_input (BatchNormaliza (None, None, 42)     168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, None, 256)    32512       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, None, 256)    54016       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv1D)                  (None, None, 256)    75520       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv1D)                  (None, None, 256)    118528      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv1D)                  (None, None, 256)    226048      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv1 (BatchNormaliz (None, None, 256)    1024        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv2 (BatchNormaliz (None, None, 256)    1024        conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv3 (BatchNormaliz (None, None, 256)    1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv4 (BatchNormaliz (None, None, 256)    1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv5 (BatchNormaliz (None, None, 256)    1024        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 1322)   0           batchnorm_input[0][0]            \n",
      "                                                                 batch_norm_conv1[0][0]           \n",
      "                                                                 batch_norm_conv2[0][0]           \n",
      "                                                                 batch_norm_conv3[0][0]           \n",
      "                                                                 batch_norm_conv4[0][0]           \n",
      "                                                                 batch_norm_conv5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_1 (Position (None, None, 1322)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, None, 400)    1586400     position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 400)    0           attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 2000)   11216000    dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 9)      18009       bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 13,332,321\n",
      "Trainable params: 13,329,677\n",
      "Non-trainable params: 2,644\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 66s 754ms/step - loss: 1.6665 - weighted_acc: 0.5302 - val_loss: 1.3476 - val_weighted_acc: 0.6052\n",
      "\n",
      "Epoch 00001: val_weighted_acc improved from -inf to 0.60517, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-01-0.530-0.605.hdf5\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 21s 243ms/step - loss: 1.1305 - weighted_acc: 0.6699 - val_loss: 1.1712 - val_weighted_acc: 0.6288\n",
      "\n",
      "Epoch 00002: val_weighted_acc improved from 0.60517 to 0.62879, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-02-0.670-0.629.hdf5\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 21s 240ms/step - loss: 0.9986 - weighted_acc: 0.6901 - val_loss: 1.0949 - val_weighted_acc: 0.6396\n",
      "\n",
      "Epoch 00003: val_weighted_acc improved from 0.62879 to 0.63961, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-03-0.690-0.640.hdf5\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 21s 242ms/step - loss: 0.9402 - weighted_acc: 0.6995 - val_loss: 1.0638 - val_weighted_acc: 0.6465\n",
      "\n",
      "Epoch 00004: val_weighted_acc improved from 0.63961 to 0.64648, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-04-0.700-0.646.hdf5\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 20s 234ms/step - loss: 0.9148 - weighted_acc: 0.7047 - val_loss: 1.0687 - val_weighted_acc: 0.6435\n",
      "\n",
      "Epoch 00005: val_weighted_acc did not improve from 0.64648\n",
      "Epoch 6/300\n",
      "87/87 [==============================] - 21s 236ms/step - loss: 0.9138 - weighted_acc: 0.7047 - val_loss: 1.0492 - val_weighted_acc: 0.6532\n",
      "\n",
      "Epoch 00006: val_weighted_acc improved from 0.64648 to 0.65323, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-06-0.705-0.653.hdf5\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 20s 233ms/step - loss: 0.8980 - weighted_acc: 0.7111 - val_loss: 1.0487 - val_weighted_acc: 0.6543\n",
      "\n",
      "Epoch 00007: val_weighted_acc improved from 0.65323 to 0.65435, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-07-0.711-0.654.hdf5\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 20s 235ms/step - loss: 0.8832 - weighted_acc: 0.7161 - val_loss: 1.0356 - val_weighted_acc: 0.6599\n",
      "\n",
      "Epoch 00008: val_weighted_acc improved from 0.65435 to 0.65993, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-08-0.717-0.660.hdf5\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 21s 240ms/step - loss: 0.8836 - weighted_acc: 0.7187 - val_loss: 1.0545 - val_weighted_acc: 0.6579\n",
      "\n",
      "Epoch 00009: val_weighted_acc did not improve from 0.65993\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 21s 240ms/step - loss: 0.9023 - weighted_acc: 0.7156 - val_loss: 1.0736 - val_weighted_acc: 0.6528\n",
      "\n",
      "Epoch 00010: val_weighted_acc did not improve from 0.65993\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 0.8969 - weighted_acc: 0.7182 - val_loss: 1.0675 - val_weighted_acc: 0.6581\n",
      "\n",
      "Epoch 00011: val_weighted_acc did not improve from 0.65993\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 21s 240ms/step - loss: 0.8860 - weighted_acc: 0.7217 - val_loss: 1.0570 - val_weighted_acc: 0.6613\n",
      "\n",
      "Epoch 00012: val_weighted_acc improved from 0.65993 to 0.66134, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-12-0.722-0.661.hdf5\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 21s 242ms/step - loss: 0.8789 - weighted_acc: 0.7257 - val_loss: 1.0628 - val_weighted_acc: 0.6618\n",
      "\n",
      "Epoch 00013: val_weighted_acc improved from 0.66134 to 0.66180, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-13-0.726-0.662.hdf5\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 21s 243ms/step - loss: 0.8775 - weighted_acc: 0.7272 - val_loss: 1.0682 - val_weighted_acc: 0.6640\n",
      "\n",
      "Epoch 00014: val_weighted_acc improved from 0.66180 to 0.66402, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-14-0.728-0.664.hdf5\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 21s 241ms/step - loss: 0.8846 - weighted_acc: 0.7264 - val_loss: 1.0709 - val_weighted_acc: 0.6649\n",
      "\n",
      "Epoch 00015: val_weighted_acc improved from 0.66402 to 0.66492, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-15-0.727-0.665.hdf5\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 20s 236ms/step - loss: 0.8820 - weighted_acc: 0.7287 - val_loss: 1.0694 - val_weighted_acc: 0.6663\n",
      "\n",
      "Epoch 00016: val_weighted_acc improved from 0.66492 to 0.66631, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-16-0.729-0.666.hdf5\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 20s 232ms/step - loss: 0.8793 - weighted_acc: 0.7296 - val_loss: 1.0855 - val_weighted_acc: 0.6621\n",
      "\n",
      "Epoch 00017: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 20s 236ms/step - loss: 0.8916 - weighted_acc: 0.7274 - val_loss: 1.0880 - val_weighted_acc: 0.6623\n",
      "\n",
      "Epoch 00018: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 20s 235ms/step - loss: 0.8893 - weighted_acc: 0.7291 - val_loss: 1.0867 - val_weighted_acc: 0.6642\n",
      "\n",
      "Epoch 00019: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 20s 234ms/step - loss: 0.8913 - weighted_acc: 0.7284 - val_loss: 1.0983 - val_weighted_acc: 0.6609\n",
      "\n",
      "Epoch 00020: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 20s 234ms/step - loss: 0.8898 - weighted_acc: 0.7297 - val_loss: 1.1000 - val_weighted_acc: 0.6656\n",
      "\n",
      "Epoch 00021: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 21s 236ms/step - loss: 0.8860 - weighted_acc: 0.7318 - val_loss: 1.0965 - val_weighted_acc: 0.6649\n",
      "\n",
      "Epoch 00022: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 21s 236ms/step - loss: 0.8865 - weighted_acc: 0.7316 - val_loss: 1.1134 - val_weighted_acc: 0.6634\n",
      "\n",
      "Epoch 00023: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 20s 235ms/step - loss: 0.8891 - weighted_acc: 0.7317 - val_loss: 1.1228 - val_weighted_acc: 0.6660\n",
      "\n",
      "Epoch 00024: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 21s 237ms/step - loss: 0.8956 - weighted_acc: 0.7298 - val_loss: 1.1225 - val_weighted_acc: 0.6661\n",
      "\n",
      "Epoch 00025: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 21s 236ms/step - loss: 0.9026 - weighted_acc: 0.7302 - val_loss: 1.1038 - val_weighted_acc: 0.6647\n",
      "\n",
      "Epoch 00026: val_weighted_acc did not improve from 0.66631\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 21s 237ms/step - loss: 0.9027 - weighted_acc: 0.7297 - val_loss: 1.1063 - val_weighted_acc: 0.6676\n",
      "\n",
      "Epoch 00027: val_weighted_acc improved from 0.66631 to 0.66763, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-27-0.730-0.668.hdf5\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 21s 236ms/step - loss: 0.9002 - weighted_acc: 0.7300 - val_loss: 1.1105 - val_weighted_acc: 0.6647\n",
      "\n",
      "Epoch 00028: val_weighted_acc did not improve from 0.66763\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 21s 240ms/step - loss: 0.8947 - weighted_acc: 0.7316 - val_loss: 1.0994 - val_weighted_acc: 0.6679\n",
      "\n",
      "Epoch 00029: val_weighted_acc improved from 0.66763 to 0.66793, saving model to results/multihead_lstm_cnnmultihead#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-29-0.732-0.668.hdf5\n",
      "Epoch 30/300\n",
      "87/87 [==============================] - 21s 238ms/step - loss: 0.9011 - weighted_acc: 0.7308 - val_loss: 1.1219 - val_weighted_acc: 0.6659\n",
      "\n",
      "Epoch 00030: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 21s 242ms/step - loss: 0.9064 - weighted_acc: 0.7297 - val_loss: 1.1121 - val_weighted_acc: 0.6640\n",
      "\n",
      "Epoch 00031: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 21s 241ms/step - loss: 0.9057 - weighted_acc: 0.7300 - val_loss: 1.1150 - val_weighted_acc: 0.6659\n",
      "\n",
      "Epoch 00032: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 22s 247ms/step - loss: 0.9163 - weighted_acc: 0.7273 - val_loss: 1.1303 - val_weighted_acc: 0.6642\n",
      "\n",
      "Epoch 00033: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 22s 248ms/step - loss: 0.9191 - weighted_acc: 0.7271 - val_loss: 1.1318 - val_weighted_acc: 0.6621\n",
      "\n",
      "Epoch 00034: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 21s 243ms/step - loss: 0.9146 - weighted_acc: 0.7292 - val_loss: 1.1350 - val_weighted_acc: 0.6636\n",
      "\n",
      "Epoch 00035: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 21s 238ms/step - loss: 0.9120 - weighted_acc: 0.7305 - val_loss: 1.1392 - val_weighted_acc: 0.6631\n",
      "\n",
      "Epoch 00036: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 21s 242ms/step - loss: 0.9164 - weighted_acc: 0.7298 - val_loss: 1.1232 - val_weighted_acc: 0.6630\n",
      "\n",
      "Epoch 00037: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 21s 241ms/step - loss: 0.9199 - weighted_acc: 0.7282 - val_loss: 1.1311 - val_weighted_acc: 0.6660\n",
      "\n",
      "Epoch 00038: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 21s 241ms/step - loss: 0.9239 - weighted_acc: 0.7277 - val_loss: 1.1466 - val_weighted_acc: 0.6622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 21s 239ms/step - loss: 0.9200 - weighted_acc: 0.7292 - val_loss: 1.1200 - val_weighted_acc: 0.6668\n",
      "\n",
      "Epoch 00040: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 21s 242ms/step - loss: 0.9236 - weighted_acc: 0.7285 - val_loss: 1.1561 - val_weighted_acc: 0.6648\n",
      "\n",
      "Epoch 00041: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 21s 241ms/step - loss: 0.9265 - weighted_acc: 0.7279 - val_loss: 1.1594 - val_weighted_acc: 0.6598\n",
      "\n",
      "Epoch 00042: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 21s 241ms/step - loss: 0.9390 - weighted_acc: 0.7253 - val_loss: 1.1698 - val_weighted_acc: 0.6599\n",
      "\n",
      "Epoch 00043: val_weighted_acc did not improve from 0.66793\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 21s 238ms/step - loss: 0.9513 - weighted_acc: 0.7229 - val_loss: 1.1646 - val_weighted_acc: 0.6583\n",
      "\n",
      "Epoch 00044: val_weighted_acc did not improve from 0.66793\n"
     ]
    }
   ],
   "source": [
    "run_transformer_attention(epochs=300, setting_name='multihead_lstm__cnn', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=15, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selfattention_modified(epochs=10, setting_name='attention', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=10, features_to_use=['onehot', 'sequence_profile'], convs=[3, 5, 7], dense_size=200, lstm_size=400,use_CRF=False):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "    # read files\n",
    "    train_file = '../DeepSeq2Sec/data/s8_all_features/train.txt'\n",
    "    test_file = '../DeepSeq2Sec/data/s8_all_features/test.txt'\n",
    "\n",
    "    LD = LabelingData(train_file, test_file)\n",
    "    train_lengths = [int(j) for j in FileUtility.load_list('/'.join(train_file.split('/')[0:-1]) + '/train_length.txt')]\n",
    "    test_lengths = [int(i) for i in FileUtility.load_list('/'.join(test_file.split('/')[0:-1]) + '/test_length.txt')]\n",
    "\n",
    "    # model\n",
    "    model, params = selfattention_model_modified(LD.n_classes, features_to_use=features_to_use, convs=convs,\n",
    "                             dense_size=dense_size, lstm_size=lstm_size,use_CRF=use_CRF, filter_size=256)\n",
    "\n",
    "    # output directory\n",
    "    FileUtility.ensure_dir('results/' + setting_name + params + '/')\n",
    "\n",
    "    # save model\n",
    "    with open('results/' + setting_name + params + '/' + 'config.txt', 'w') as fh:\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    # check points\n",
    "    filepath = 'results/' + setting_name + params + \"/weights-improvement-{epoch:02d}-{weighted_acc:.3f}-{val_weighted_acc:.3f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_weighted_acc', verbose=1, save_best_only=True, mode='max',\n",
    "                                 period=1)\n",
    "    earlystopping = EarlyStopping(monitor='val_weighted_acc', min_delta=0, patience=patience, verbose=0, mode='max',\n",
    "                                  baseline=None)\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    # calculate the sizes\n",
    "    steps_per_epoch = len(train_lengths) / train_batch_size if len(train_lengths) % train_batch_size == 0 else int(\n",
    "        len(train_lengths) / train_batch_size) + 1\n",
    "    validation_steps = int(len(test_lengths) / test_batch_size) if len(test_lengths) % test_batch_size == 0 else int(\n",
    "        len(test_lengths) / test_batch_size) + 1\n",
    "\n",
    "    # feed model\n",
    "    h = model.fit_generator(train_batch_generator_408(train_batch_size), steps_per_epoch=steps_per_epoch,\n",
    "                            validation_data=validation_batch_generator_408(test_batch_size),\n",
    "                            validation_steps=validation_steps,\n",
    "                            shuffle=False, epochs=epochs, verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    # save the history\n",
    "    FileUtility.save_obj('results/' + setting_name + params + '/history', h.history)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling utility object created..\n",
      "Training y encoded shape is  (5534, 700)\n",
      "Maximum sequence length is 700\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 408)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "onehot (Lambda)                 (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequenceprofile (Lambda)        (None, None, 21)     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 42)     0           onehot[0][0]                     \n",
      "                                                                 sequenceprofile[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_input (BatchNormaliza (None, None, 42)     168         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, None, 256)    32512       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, None, 256)    54016       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv1D)                  (None, None, 256)    75520       batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv1D)                  (None, None, 256)    118528      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5 (Conv1D)                  (None, None, 256)    226048      batchnorm_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv1 (BatchNormaliz (None, None, 256)    1024        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv2 (BatchNormaliz (None, None, 256)    1024        conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv3 (BatchNormaliz (None, None, 256)    1024        conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv4 (BatchNormaliz (None, None, 256)    1024        conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_conv5 (BatchNormaliz (None, None, 256)    1024        conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 1322)   0           batchnorm_input[0][0]            \n",
      "                                                                 batch_norm_conv1[0][0]           \n",
      "                                                                 batch_norm_conv2[0][0]           \n",
      "                                                                 batch_norm_conv3[0][0]           \n",
      "                                                                 batch_norm_conv4[0][0]           \n",
      "                                                                 batch_norm_conv5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropoutonconvs (Dropout)        (None, None, 1322)   0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "denseonconvs (Dense)            (None, None, 1000)   1323000     dropoutonconvs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_dense (BatchNormaliz (None, None, 1000)   4000        denseonconvs[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 1000)   0           batch_norm_dense[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 2000)   16016000    dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Attention (SeqSelfAttention)    (None, None, 2000)   4000001     bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 2000)   0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 65)     130065      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 9)      594         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,985,572\n",
      "Trainable params: 21,980,928\n",
      "Non-trainable params: 4,644\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/300\n",
      "87/87 [==============================] - 73s 841ms/step - loss: 1.6670 - weighted_acc: 0.5981 - val_loss: 1.4805 - val_weighted_acc: 0.6230\n",
      "\n",
      "Epoch 00001: val_weighted_acc improved from -inf to 0.62300, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-01-0.598-0.623.hdf5\n",
      "Epoch 2/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 1.2454 - weighted_acc: 0.6801 - val_loss: 1.2907 - val_weighted_acc: 0.6260\n",
      "\n",
      "Epoch 00002: val_weighted_acc improved from 0.62300 to 0.62605, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-02-0.680-0.626.hdf5\n",
      "Epoch 3/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 1.0488 - weighted_acc: 0.6992 - val_loss: 1.1771 - val_weighted_acc: 0.6305\n",
      "\n",
      "Epoch 00003: val_weighted_acc improved from 0.62605 to 0.63047, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-03-0.700-0.630.hdf5\n",
      "Epoch 4/300\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.9391 - weighted_acc: 0.7098 - val_loss: 1.1122 - val_weighted_acc: 0.6362\n",
      "\n",
      "Epoch 00004: val_weighted_acc improved from 0.63047 to 0.63624, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-04-0.710-0.636.hdf5\n",
      "Epoch 5/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.8847 - weighted_acc: 0.7176 - val_loss: 1.0311 - val_weighted_acc: 0.6629\n",
      "\n",
      "Epoch 00005: val_weighted_acc improved from 0.63624 to 0.66290, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-05-0.718-0.663.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.8481 - weighted_acc: 0.7240 - val_loss: 1.0051 - val_weighted_acc: 0.6662\n",
      "\n",
      "Epoch 00006: val_weighted_acc improved from 0.66290 to 0.66624, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-06-0.724-0.666.hdf5\n",
      "Epoch 7/300\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.8227 - weighted_acc: 0.7284 - val_loss: 0.9876 - val_weighted_acc: 0.6713\n",
      "\n",
      "Epoch 00007: val_weighted_acc improved from 0.66624 to 0.67134, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-07-0.729-0.671.hdf5\n",
      "Epoch 8/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.8153 - weighted_acc: 0.7319 - val_loss: 0.9727 - val_weighted_acc: 0.6780\n",
      "\n",
      "Epoch 00008: val_weighted_acc improved from 0.67134 to 0.67797, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-08-0.732-0.678.hdf5\n",
      "Epoch 9/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 0.7942 - weighted_acc: 0.7368 - val_loss: 0.9575 - val_weighted_acc: 0.6807\n",
      "\n",
      "Epoch 00009: val_weighted_acc improved from 0.67797 to 0.68074, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-09-0.737-0.681.hdf5\n",
      "Epoch 10/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7840 - weighted_acc: 0.7403 - val_loss: 0.9540 - val_weighted_acc: 0.6840\n",
      "\n",
      "Epoch 00010: val_weighted_acc improved from 0.68074 to 0.68405, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-10-0.741-0.684.hdf5\n",
      "Epoch 11/300\n",
      "87/87 [==============================] - 26s 301ms/step - loss: 0.7811 - weighted_acc: 0.7430 - val_loss: 0.9513 - val_weighted_acc: 0.6882\n",
      "\n",
      "Epoch 00011: val_weighted_acc improved from 0.68405 to 0.68815, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-11-0.743-0.688.hdf5\n",
      "Epoch 12/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7695 - weighted_acc: 0.7465 - val_loss: 0.9453 - val_weighted_acc: 0.6869\n",
      "\n",
      "Epoch 00012: val_weighted_acc did not improve from 0.68815\n",
      "Epoch 13/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7652 - weighted_acc: 0.7491 - val_loss: 0.9561 - val_weighted_acc: 0.6895\n",
      "\n",
      "Epoch 00013: val_weighted_acc improved from 0.68815 to 0.68947, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-13-0.750-0.689.hdf5\n",
      "Epoch 14/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.7634 - weighted_acc: 0.7513 - val_loss: 0.9476 - val_weighted_acc: 0.6895\n",
      "\n",
      "Epoch 00014: val_weighted_acc improved from 0.68947 to 0.68955, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-14-0.752-0.690.hdf5\n",
      "Epoch 15/300\n",
      "87/87 [==============================] - 26s 303ms/step - loss: 0.7420 - weighted_acc: 0.7557 - val_loss: 0.9441 - val_weighted_acc: 0.6900\n",
      "\n",
      "Epoch 00015: val_weighted_acc improved from 0.68955 to 0.69000, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-15-0.756-0.690.hdf5\n",
      "Epoch 16/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.7578 - weighted_acc: 0.7561 - val_loss: 0.9668 - val_weighted_acc: 0.6912\n",
      "\n",
      "Epoch 00016: val_weighted_acc improved from 0.69000 to 0.69117, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-16-0.757-0.691.hdf5\n",
      "Epoch 17/300\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.7411 - weighted_acc: 0.7600 - val_loss: 0.9495 - val_weighted_acc: 0.6886\n",
      "\n",
      "Epoch 00017: val_weighted_acc did not improve from 0.69117\n",
      "Epoch 18/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.7281 - weighted_acc: 0.7647 - val_loss: 0.9572 - val_weighted_acc: 0.6914\n",
      "\n",
      "Epoch 00018: val_weighted_acc improved from 0.69117 to 0.69136, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-18-0.765-0.691.hdf5\n",
      "Epoch 19/300\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.7177 - weighted_acc: 0.7665 - val_loss: 0.9473 - val_weighted_acc: 0.6922\n",
      "\n",
      "Epoch 00019: val_weighted_acc improved from 0.69136 to 0.69216, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-19-0.767-0.692.hdf5\n",
      "Epoch 20/300\n",
      "87/87 [==============================] - 26s 301ms/step - loss: 0.7298 - weighted_acc: 0.7681 - val_loss: 0.9807 - val_weighted_acc: 0.6921\n",
      "\n",
      "Epoch 00020: val_weighted_acc did not improve from 0.69216\n",
      "Epoch 21/300\n",
      "87/87 [==============================] - 26s 300ms/step - loss: 0.7173 - weighted_acc: 0.7707 - val_loss: 0.9627 - val_weighted_acc: 0.6919\n",
      "\n",
      "Epoch 00021: val_weighted_acc did not improve from 0.69216\n",
      "Epoch 22/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 0.7089 - weighted_acc: 0.7722 - val_loss: 1.0275 - val_weighted_acc: 0.6854\n",
      "\n",
      "Epoch 00022: val_weighted_acc did not improve from 0.69216\n",
      "Epoch 23/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 0.7636 - weighted_acc: 0.7657 - val_loss: 0.9789 - val_weighted_acc: 0.6922\n",
      "\n",
      "Epoch 00023: val_weighted_acc improved from 0.69216 to 0.69222, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-23-0.766-0.692.hdf5\n",
      "Epoch 24/300\n",
      "87/87 [==============================] - 26s 302ms/step - loss: 0.7259 - weighted_acc: 0.7735 - val_loss: 0.9839 - val_weighted_acc: 0.6910\n",
      "\n",
      "Epoch 00024: val_weighted_acc did not improve from 0.69222\n",
      "Epoch 25/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.6989 - weighted_acc: 0.7792 - val_loss: 0.9795 - val_weighted_acc: 0.6921\n",
      "\n",
      "Epoch 00025: val_weighted_acc did not improve from 0.69222\n",
      "Epoch 26/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.6941 - weighted_acc: 0.7807 - val_loss: 0.9892 - val_weighted_acc: 0.6940\n",
      "\n",
      "Epoch 00026: val_weighted_acc improved from 0.69222 to 0.69404, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-26-0.781-0.694.hdf5\n",
      "Epoch 27/300\n",
      "87/87 [==============================] - 26s 304ms/step - loss: 0.6803 - weighted_acc: 0.7839 - val_loss: 0.9621 - val_weighted_acc: 0.6928\n",
      "\n",
      "Epoch 00027: val_weighted_acc did not improve from 0.69404\n",
      "Epoch 28/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.6582 - weighted_acc: 0.7877 - val_loss: 0.9691 - val_weighted_acc: 0.6904\n",
      "\n",
      "Epoch 00028: val_weighted_acc did not improve from 0.69404\n",
      "Epoch 29/300\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.6535 - weighted_acc: 0.7891 - val_loss: 0.9691 - val_weighted_acc: 0.6956\n",
      "\n",
      "Epoch 00029: val_weighted_acc improved from 0.69404 to 0.69558, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-29-0.790-0.696.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.6715 - weighted_acc: 0.7900 - val_loss: 1.0278 - val_weighted_acc: 0.6921\n",
      "\n",
      "Epoch 00030: val_weighted_acc did not improve from 0.69558\n",
      "Epoch 31/300\n",
      "87/87 [==============================] - 27s 305ms/step - loss: 0.7122 - weighted_acc: 0.7880 - val_loss: 1.0275 - val_weighted_acc: 0.6964\n",
      "\n",
      "Epoch 00031: val_weighted_acc improved from 0.69558 to 0.69642, saving model to results/self-attention_withconv_2drops_dense_normselfattention#onehot#sequence_profile@conv3_5_7_11_21@dense_1000@lstm1000@droplstm0.5@filtersize_256/weights-improvement-31-0.789-0.696.hdf5\n",
      "Epoch 32/300\n",
      "87/87 [==============================] - 27s 309ms/step - loss: 0.6604 - weighted_acc: 0.7953 - val_loss: 0.9920 - val_weighted_acc: 0.6917\n",
      "\n",
      "Epoch 00032: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 33/300\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.6315 - weighted_acc: 0.7981 - val_loss: 0.9738 - val_weighted_acc: 0.6927\n",
      "\n",
      "Epoch 00033: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 34/300\n",
      "87/87 [==============================] - 27s 307ms/step - loss: 0.6247 - weighted_acc: 0.8011 - val_loss: 0.9845 - val_weighted_acc: 0.6919\n",
      "\n",
      "Epoch 00034: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 35/300\n",
      "87/87 [==============================] - 27s 310ms/step - loss: 0.6357 - weighted_acc: 0.8016 - val_loss: 1.0404 - val_weighted_acc: 0.6938\n",
      "\n",
      "Epoch 00035: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 36/300\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.6273 - weighted_acc: 0.8051 - val_loss: 1.0347 - val_weighted_acc: 0.6921\n",
      "\n",
      "Epoch 00036: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 37/300\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.6047 - weighted_acc: 0.8080 - val_loss: 0.9878 - val_weighted_acc: 0.6920\n",
      "\n",
      "Epoch 00037: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 38/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.5888 - weighted_acc: 0.8118 - val_loss: 0.9884 - val_weighted_acc: 0.6936\n",
      "\n",
      "Epoch 00038: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 39/300\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.5885 - weighted_acc: 0.8145 - val_loss: 1.0138 - val_weighted_acc: 0.6924\n",
      "\n",
      "Epoch 00039: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 40/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.5881 - weighted_acc: 0.8164 - val_loss: 1.0208 - val_weighted_acc: 0.6884\n",
      "\n",
      "Epoch 00040: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 41/300\n",
      "87/87 [==============================] - 27s 313ms/step - loss: 0.5760 - weighted_acc: 0.8187 - val_loss: 1.0345 - val_weighted_acc: 0.6867\n",
      "\n",
      "Epoch 00041: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 42/300\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.5676 - weighted_acc: 0.8230 - val_loss: 1.0474 - val_weighted_acc: 0.6854\n",
      "\n",
      "Epoch 00042: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 43/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.5564 - weighted_acc: 0.8264 - val_loss: 1.0395 - val_weighted_acc: 0.6833\n",
      "\n",
      "Epoch 00043: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 44/300\n",
      "87/87 [==============================] - 27s 306ms/step - loss: 0.5349 - weighted_acc: 0.8305 - val_loss: 1.0339 - val_weighted_acc: 0.6844\n",
      "\n",
      "Epoch 00044: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 45/300\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.5496 - weighted_acc: 0.8327 - val_loss: 1.0824 - val_weighted_acc: 0.6868\n",
      "\n",
      "Epoch 00045: val_weighted_acc did not improve from 0.69642\n",
      "Epoch 46/300\n",
      "87/87 [==============================] - 27s 308ms/step - loss: 0.5410 - weighted_acc: 0.8357 - val_loss: 1.0568 - val_weighted_acc: 0.6864\n",
      "\n",
      "Epoch 00046: val_weighted_acc did not improve from 0.69642\n"
     ]
    }
   ],
   "source": [
    "model=run_selfattention_modified(epochs=300, setting_name='self-attention_withconv_2drops_dense_instance_norm_', gpu='0', train_batch_size=64,\n",
    "                 test_batch_size=100, patience=15, features_to_use=['onehot', 'sequence_profile'], convs=[3,5,7,11,21], dense_size=1000, lstm_size=1000,use_CRF=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
